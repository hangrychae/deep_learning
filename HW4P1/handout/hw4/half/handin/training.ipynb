{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests_hw4 import test_prediction, test_generation\n",
        "\n",
        "\n",
        "from torch.optim import Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('../dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        \n",
        "        self.data = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seq_len = 70.0\n",
        "\n",
        "\n",
        "        \n",
        "    def BPTT(self, i, x, y):\n",
        "        init_bptt = self.seq_len\n",
        "        bptt = self.seq_len if np.random.random() < 0.95 else self.seq_len/2\n",
        "        seq_len = int(np.random.normal(bptt, 5))\n",
        "        if seq_len == 0: seq_len = self.seq_len\n",
        "        \n",
        "        x, y = x[:,i:i+seq_len], y[:,i:i+seq_len]\n",
        "        x, y = torch.tensor(x).int(), torch.LongTensor(y)\n",
        "        i += seq_len\n",
        "\n",
        "        return x, y, init_bptt, seq_len, i\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        # concatenate your articles and build into batches\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.data)\n",
        "        conc_article = np.concatenate(self.data)\n",
        "        nbatch = conc_article.shape[0] // self.batch_size\n",
        "        trimmed_data = conc_article[:nbatch*self.batch_size+1]\n",
        "        inputs = trimmed_data[:-1].reshape((self.batch_size, -1))\n",
        "        targets = trimmed_data[1:].reshape((self.batch_size, -1))\n",
        "        i = 0\n",
        "        while i < nbatch:\n",
        "          x, y, init_bptt, seq_len, i = self.BPTT(i, inputs, targets)\n",
        "          yield x, y, init_bptt, seq_len\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.dropout1 = nn.Dropout(p=0.4)\n",
        "        self.embedding = nn.Embedding(vocab_size, 400)\n",
        "        self.dropout2 = nn.Dropout(p=0.1)\n",
        "        # self.lstm = nn.LSTM(input_size=400, hidden_size=1150, num_layers=3, dropout=0.3, batch_first=True)\n",
        "        self.lstm = nn.LSTM(input_size=400, hidden_size=1150, num_layers=4, batch_first=True)\n",
        "        self.dropout3 = nn.Dropout(p=0.4)\n",
        "        self.classification = nn.Linear(1150, vocab_size)\n",
        "\n",
        "    def init_weight(self, model):\n",
        "        # pass\n",
        "        if type(model) == nn.Embedding:\n",
        "          nn.init.uniform_(model.weight, a=-0.1, b=0.1)\n",
        "        elif type(model) == nn.LSTM:\n",
        "          for name, para in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "              nn.init.uniform_(para, a=-1/33.9116499156, b=1/33.9116499156)\n",
        "            else: #bias\n",
        "              nn.init.constant_(para, 0.0)\n",
        "\n",
        "    def forward(self, x, hn=None):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        # dropword = self.dropout1(x)\n",
        "        # embedding = self.embedding(dropword)\n",
        "        # dropemb = self.dropout2(embedding)\n",
        "        # output, (hn, cn) = self.lstm(embedding) if hn==None else self.lstm(embedding, hn) \n",
        "        # output = self.dropout3(output)\n",
        "        # output = self.classification(output)\n",
        "        \n",
        "        embedding = self.embedding(x)\n",
        "        output, (hn, cn) = self.lstm(embedding) if hn==None else self.lstm(embedding, hn) \n",
        "        output = self.classification(output)\n",
        "        return output, (hn, cn)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        self.lr = 2e-3\n",
        "        # self.lr = 30\n",
        "        self.weight_decay = 5e-4\n",
        "        self.alpha = 2\n",
        "        self.beta = 1\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    def train(self):\n",
        "        # self.model.apply(self.model.init_weight)\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        # for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "        for batch_num, (x, y, init_bptt, seq_len) in enumerate(self.loader):\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "            # self.optimizer.param_groups[0]['lr'] *= seq_len / init_bptt\n",
        "            epoch_loss += self.train_batch(x, y)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "        \n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():    \n",
        "            outputs, (hn, cn) = self.model(inputs) #enforce_sorted=False)\n",
        "            outputs = torch.reshape(outputs, (-1, outputs.size(2)))\n",
        "            targets = torch.flatten(targets)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "        self.scaler.scale(loss).backward() \n",
        "        # nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.25, norm_type=2)\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        \n",
        "                \n",
        "        # outputs, (hn, cn) = self.model(inputs) #enforce_sorted=False)\n",
        "        # outputs = torch.reshape(outputs, (-1, outputs.size(2)))\n",
        "        # targets = torch.flatten(targets)\n",
        "        # loss = self.criterion(outputs, targets)\n",
        "        # loss.backward()\n",
        "        # nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.25, norm_type=2)\n",
        "        # self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model, h=None):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        x = torch.cuda.LongTensor(inp)\n",
        "        outputs, (hn, cn)= model(x) if h==None else model(x, h)\n",
        "        return outputs[:,-1].cpu().detach().numpy()\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"   \n",
        "        model.eval()\n",
        "        gen_seq = []\n",
        " \n",
        "        with torch.no_grad():\n",
        "            x = torch.cuda.LongTensor(inp)\n",
        "            for i in range(forward):\n",
        "              if i == 0:\n",
        "                outputs, (hn, cn) = model(x)\n",
        "              else:\n",
        "                outputs, (hn, cn) = model(x, (hn, cn))\n",
        "              outputs = outputs[:,-1]\n",
        "              word = torch.argmax(outputs, dim=1)\n",
        "              word = word.cpu().detach().numpy()\n",
        "              gen_seq.append(word)\n",
        "\n",
        "        result = np.array(gen_seq).T\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "# NUM_EPOCHS = 750\n",
        "NUM_EPOCHS = 20\n",
        "# BATCH_SIZE = 80\n",
        "BATCH_SIZE = 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2HCVG5YISwRW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1651507520\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000010?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m LanguageModel(\u001b[39mlen\u001b[39;49m(vocab))\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000010?line=1'>2</a>\u001b[0m loader \u001b[39m=\u001b[39m LanguageModelDataLoader(dataset\u001b[39m=\u001b[39mdataset, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000010?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m LanguageModelTrainer(model\u001b[39m=\u001b[39mmodel, loader\u001b[39m=\u001b[39mloader, max_epochs\u001b[39m=\u001b[39mNUM_EPOCHS, run_id\u001b[39m=\u001b[39mrun_id)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:680\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=662'>663</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=663'>664</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=664'>665</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=665'>666</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=677'>678</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=678'>679</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=679'>680</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=568'>569</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=569'>570</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=571'>572</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=572'>573</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=573'>574</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=574'>575</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=589'>590</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=590'>591</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=591'>592</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=592'>593</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=593'>594</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=594'>595</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:680\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=662'>663</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=663'>664</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=664'>665</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=665'>666</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=677'>678</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=678'>679</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=679'>680</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py:208\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=203'>204</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=204'>205</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=205'>206</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/chae/opt/anaconda3/envs/env/lib/python3.9/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "model = LanguageModel(len(vocab)).cuda()\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D8wTJkBSwRc"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "iter() returned non-iterator of type 'tuple'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000009?line=0'>1</a>\u001b[0m best_nll \u001b[39m=\u001b[39m \u001b[39m1e30\u001b[39m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000009?line=2'>3</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000009?line=3'>4</a>\u001b[0m     nll \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000009?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m nll \u001b[39m<\u001b[39m best_nll:\n",
            "\u001b[1;32m/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb Cell 7'\u001b[0m in \u001b[0;36mLanguageModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000004?line=46'>47</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000004?line=47'>48</a>\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000004?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000004?line=49'>50</a>\u001b[0m     x, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mseq_len \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBPTT(inputs, targets, batch_num, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mseq_len)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chae/Desktop/Spring22/11685/hw4/handout/hw4/training.ipynb#ch0000004?line=50'>51</a>\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_batch(x, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mseq_len)\n",
            "\u001b[0;31mTypeError\u001b[0m: iter() returned non-iterator of type 'tuple'"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2FmDqBCSwRf"
      },
      "outputs": [],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipdbmqaGSwRh"
      },
      "outputs": [],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
